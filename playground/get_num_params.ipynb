{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获得一个 mmrotate 模型的参数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing rotated-fcos-le90_r50_fpn_1x_dota...\n",
      "\u001b[32mrotated_fcos_r50_fpn_1x_dota_le90-d87568ed.pth exists in /mnt/petrelfs/liqingyun/florence-dota/scripts_py/eval_rfcos/rotated-fcos-le90_r50_fpn_1x_dota\u001b[0m\n",
      "\u001b[32mSuccessfully dumped rotated-fcos-le90_r50_fpn_1x_dota.py to /mnt/petrelfs/liqingyun/florence-dota/scripts_py/eval_rfcos/rotated-fcos-le90_r50_fpn_1x_dota\u001b[0m\n",
      "rotated-fcos-le90_r50_fpn_1x_dota #params: 32,147,803\n",
      "processing rotated-retinanet-rbox-le90_r50_fpn_1x_dota...\n",
      "\u001b[32mrotated_retinanet_obb_r50_fpn_1x_dota_le90-c0097bc4.pth exists in /mnt/petrelfs/liqingyun/florence-dota/scripts_py/eval_mmrotate/rotated-retinanet-rbox-le90_r50_fpn_1x_dota/rotated-retinanet-rbox-le90_r50_fpn_1x_dota\u001b[0m\n",
      "\u001b[32mSuccessfully dumped rotated-retinanet-rbox-le90_r50_fpn_1x_dota.py to /mnt/petrelfs/liqingyun/florence-dota/scripts_py/eval_mmrotate/rotated-retinanet-rbox-le90_r50_fpn_1x_dota/rotated-retinanet-rbox-le90_r50_fpn_1x_dota\u001b[0m\n",
      "rotated-retinanet-rbox-le90_r50_fpn_1x_dota #params: 36,641,012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import mim\n",
    "import torch\n",
    "\n",
    "from mmengine.config import Config\n",
    "from mmengine.registry import MODELS\n",
    "from mmrotate.utils import register_all_modules\n",
    "from mmdet.utils import register_all_modules as register_all_modules_mmdet\n",
    "\n",
    "\n",
    "def monkey_patch_of_collections_typehint_for_mmrotate1x():\n",
    "    import collections\n",
    "    from collections.abc import Mapping, Sequence, Iterable\n",
    "    collections.Mapping = Mapping\n",
    "    collections.Sequence = Sequence\n",
    "    collections.Iterable = Iterable\n",
    "\n",
    "def get_num_parameters(module):\n",
    "    \"\"\"Modified from print_trainable_parameters of peft\"\"\"\n",
    "    def _get_parameter_numel(param):\n",
    "        num_params = param.numel()\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            # if using DS Zero 3 and the weights are initialized empty\n",
    "            num_params = param.ds_numel\n",
    "        return num_params\n",
    "    \n",
    "    if isinstance(module, torch.Tensor):  # nn.Parameter()\n",
    "        num_params = _get_parameter_numel(module)\n",
    "        return num_params if module.requires_grad else 0, num_params\n",
    "        \n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for param in module.parameters():\n",
    "        num_params = _get_parameter_numel(param)\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    return trainable_params, all_param\n",
    "\n",
    "monkey_patch_of_collections_typehint_for_mmrotate1x()\n",
    "\n",
    "register_all_modules_mmdet(init_default_scope=False)\n",
    "register_all_modules(init_default_scope=True)\n",
    "\n",
    "def print_mmrotate_model_num_params(model_name):\n",
    "    save_root = f\"./eval_mmrotate/{model_name}\"\n",
    "    mim.download('mmrotate', [model_name], dest_root=os.path.join(save_root, model_name))[0]\n",
    "    cfg_fpath = os.path.join(save_root, model_name, f'{model_name}.py')\n",
    "    cfg = Config.fromfile(cfg_fpath)\n",
    "    model = MODELS.build(cfg.model)\n",
    "\n",
    "    num_params = get_num_parameters(model)[1]\n",
    "    print(f\"{model_name} #params: {num_params:,d}\")\n",
    "\n",
    "print_mmrotate_model_num_params(model_name='rotated-fcos-le90_r50_fpn_1x_dota')\n",
    "print_mmrotate_model_num_params(model_name='rotated-retinanet-rbox-le90_r50_fpn_1x_dota')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获得一个 huggingface transformers 模型额参数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoModel\n",
    "\n",
    "def get_num_parameters(module):\n",
    "    \"\"\"Modified from print_trainable_parameters of peft\"\"\"\n",
    "    def _get_parameter_numel(param):\n",
    "        num_params = param.numel()\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            # if using DS Zero 3 and the weights are initialized empty\n",
    "            num_params = param.ds_numel\n",
    "        return num_params\n",
    "    \n",
    "    if isinstance(module, torch.Tensor):  # nn.Parameter()\n",
    "        num_params = _get_parameter_numel(module)\n",
    "        return num_params if module.requires_grad else 0, num_params\n",
    "        \n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for param in module.parameters():\n",
    "        num_params = _get_parameter_numel(param)\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    return trainable_params, all_param\n",
    "\n",
    "def print_transformer_model_num_params(model_name):\n",
    "    if \"Florence\" in model_name:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, attn_implementation=\"eager\", trust_remote_code=True)\n",
    "    elif \"deepseek\" in model_name:\n",
    "        from lmmrotate.models.deepseek_vl2 import DeepseekVLV2ForCausalLM\n",
    "        model = DeepseekVLV2ForCausalLM.from_pretrained(model_name, attn_implementation=\"eager\", trust_remote_code=True)\n",
    "    elif \"llava\" in model_name:\n",
    "        from lmmrotate.models.llava import load_pretrained_model\n",
    "        model = load_pretrained_model(model_path=model_name, model_name=\"llava_qwen\")[1]\n",
    "    else:\n",
    "        model = AutoModel.from_pretrained(model_name, attn_implementation=\"eager\", trust_remote_code=True)\n",
    "    num_params = get_num_parameters(model)[1]\n",
    "    print(f\"{model_name} #params: {num_params:,d}\")\n",
    "\n",
    "print_transformer_model_num_params(model_name='microsoft/Florence-2-base')  # huggingface-cli download microsoft/Florence-2-base --repo-type model\n",
    "print_transformer_model_num_params(model_name='microsoft/Florence-2-large')  # huggingface-cli download microsoft/Florence-2-large --repo-type model\n",
    "print_transformer_model_num_params(model_name='Qwen/Qwen2-VL-2B-Instruct')  # huggingface-cli download Qwen/Qwen2-VL-2B-Instruct --repo-type model\n",
    "print_transformer_model_num_params(model_name='lmms-lab/llava-onevision-qwen2-0.5b-si')  # huggingface-cli download lmms-lab/llava-onevision-qwen2-0.5b-si --repo-type model\n",
    "print_transformer_model_num_params(model_name='OpenGVLab/InternVL2-1B')  # huggingface-cli download OpenGVLab/InternVL2-1B --repo-type model\n",
    "print_transformer_model_num_params(model_name='OpenGVLab/InternVL2-2B')  # huggingface-cli download OpenGVLab/InternVL2-2B --repo-type model\n",
    "print_transformer_model_num_params(model_name='OpenGVLab/InternVL2-8B')  # huggingface-cli download OpenGVLab/InternVL2-8B --repo-type model\n",
    "\n",
    "# microsoft/Florence-2-base #params: 270,803,968\n",
    "# microsoft/Florence-2-large #params: 828,985,344\n",
    "# Qwen/Qwen2-VL-2B-Instruct #params: 1,543,714,304\n",
    "# lmms-lab/llava-onevision-qwen2-0.5b-si #params: 893,359,264\n",
    "# OpenGVLab/InternVL2-1B #params: 938,158,976\n",
    "# OpenGVLab/InternVL2-2B #params: 2,205,754,368\n",
    "# OpenGVLab/InternVL2-8B #params: 8,075,365,376"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "florence-dota",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
